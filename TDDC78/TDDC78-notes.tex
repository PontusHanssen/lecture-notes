%        File: TDDC78-lecture-notes.tex
%     Created: Mon Apr 04 08:00  2016 C
% Last Change: Mon Apr 04 08:00  2016 C
%
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{hyperref}
\author{Pontus Persson}
\title{Lecture Notes\\TDDC78}
\date{VT-16}
\begin{document}
\maketitle
\tableofcontents

\marginpar{Lecture 1\\2016-04-04}
\section{Parallel Computing - Motivation}
Very large number of FLOPS. Used for example weather predictions. 10 day forecast
requires $10^{15}$ FLOPS.
\\Grand challenges:
\begin{itemize}
    \item Chemistry
    \item Physics
    \item VR
    \item Bioinformatics
\end{itemize}
Single processor computers cannot provide such massive computation power
\begin{itemize}
    \item Power/Cooling issue (freq. limited)
    \item Limited automatic parallelism (eg. at instruction level)
\end{itemize}
It is more efficient to use multiple cores instead of simply over-clocking a single core.
Given that the application can be parallelized efficiently.

\section{Parallel Computer Architecture Concepts}
\begin{description}
    \item[Multiprocessors] Tightly connected processors (eg shared memory)
    \item[Multicomputer] Loosely connected (eg distributed memory)
\end{description}
\subsection{Control Structure}
\begin{description}
    \item[SISD] single instruction stream, single data stream\\ sequential
    \item[SIMD] single instruction stream, multiple data streams\\
        Common clock, common program memory, common program counter
        \begin{itemize}
            \item VLIW processors
            \item traditional vector processors
            \item traditional array computers
            \item SIMD instructions on wide data words (e.g. Altivec, SSE, \ldots)
        \end{itemize}
    \item[MIMD] multiple instruction streams, multiple data streams. Each processors has its own program counter
    \item[Hybrid forms]
\end{description}

\subsection{Memory organization}
Memory access is a bottleneck in computing, caches are valuable!
\begin{description}
    \item[Distributed] No direct access, but through inter-connection network
        (message passing)
    \item[Shared] Direct access to memory from each processor via e.g. a bus.
\end{description}
Most common today in HPC: Hybrid memory systems. Distributed memory systems with
shared memory nodes. Clusters of servers with multicore CPUs.

\subsection{Distributed Memory Architectures}
\subsubsection{Array Computers}
\begin{itemize}
    \item SIMD
    \item Channel based, synchronous communication to neighbours
    \item Out of fashion, but comeback as SIMD accelerator arrays on chip.
\end{itemize}

\subsubsection{Multicomputers}
\begin{itemize}
    \item MIMD, async.
    \item may consist of standard PC components
    \item Very successful
\end{itemize}
Data structures must be distributed since remote memory access is very expensive.

\section{Interconnection Networks}
\begin{description}
    \item[Degree] max number of neighbours
    \item[Diameter] max distance in hops
    \item[Connectivity] fault tolerance
    \item[Bandwidth] max bytes/sec
    \item[throughput] max bytes/sec of all connections are active
\end{description}
We can:
\begin{itemize}
    \item Connect processors with each other (DMS, dynamic)
    \item Connect processors with memory modules (SMS, static)
\end{itemize}

Classification:
\begin{itemize}
    \item Direct/Static interconnection networks
    \item Direct networks with hardware routers
        \begin{itemize}
            \item Offload processors from most communication work
        \end{itemize}
    \item Switches/Dynamic interconnection networks
\end{itemize}

\subsection{Simple topology}
\begin{description}
    \item[Bus] 1 wire - bus saturated with many processors (Ethernet)
    \item[Linear Array] Like a linked list of processors.
    \item[Ring] Token ring
    \item[2D grid]
    \item[Torus] 2D grid + ring mixed
    \item[3D grid]
    \item[3D torus] 3D grid + torus mix
    \item[Tree] represent as tree, root processor is bottleneck
\end{description}
Could also use fat trees - trees with nodes interconnected to all parent level nodes.

\subsubsection{Hypercube}
A k-dimensional hypercube $C_k$ has $2^k$ nodes (connections). 
Inductive definition $C_0=$ one elem, $C_1=$line, $C_2=$square, $C_3=$cube, \ldots\\
$C_k=$two connected $C_{k-1}$ hypercubes.
\begin{itemize}
    \item Each node has a degree k (nmbr of direct neighbours)
    \item $\forall (p_i,p_j) \exists$ path of lengths $\leq k$ hops\\
        Shortest path $p_i \rightarrow^* p_j$ has length $HD(p_j,p_i)=$ \# 1-bits in 
\end{itemize}
\subsubsection{Crossbar}
Links every input to every output using one switch per connection. Total $n^2$ switches.

\begin{itemize}
    \item Very flexible
    \item High throughput
    \item Expensive
    \item Not scalable
\end{itemize}
Sometimes used in combination with other networks. Crossbars of 32 nodes connected
 with ring network.

 \section{Shared memory}
\marginpar{Lecture 2\\2016-04-05}
 \begin{itemize}
     \item SMP (Symmetric SMS) true shared
     \item SMP with cahces
     \item Distributed Shared Memory (DMS/NUMA) emulated on top of distributed memory
 \end{itemize}
 \section{Cache}
 Small, fast memory between processor and main memory.
 \begin{description}
     \item[cache hit] accessed work is in cache
     \item[cache miss] not in cache
     \item[cache line] holds a copy of a block of adhjacent memory words
         \begin{itemize}
             \item size: from 16 bytes and upwards
         \end{itemize}
 \end{description}
Time to access n bytes: $t(n)=\beta+\alpha n$. Depending on $\alpha$ and $\beta$,
accessing more bytes might be more efficient.
\\Mapping memory blocks to cache lines:
directly mapped $\forall j\exists!i: B_j\rightarrow C_i$ namely where\ldots
\begin{itemize}
    \item Spatial access locality
        \begin{itemize}
            \item access also other data in same cache line
        \end{itemize}
        \item temporal access locality
            \begin{itemize}
                \item access same location multiple times
            \end{itemize}
        \item HW-controlled cache line replacement
            \begin{itemize}
                \item LRU - least recently used - dynamic adaptivity of cache contents
            \end{itemize}
        \item Suitable for applications with high or dynamic data locality
\end{itemize}
\subsection{Cache misses}
\begin{description}
    \item[Cold miss] data was never in cache
        \begin{itemize}
            \item Usually unavoidable
        \end{itemize}
    \item[Capacity miss] data was evicted earlier when cache capacity was exceeded
        \begin{itemize}
            \item Might be partly avoided
        \end{itemize}
    \item[Associativity miss] data was evicted earlier because of cache set conflict
\end{description}
\subsection{Memory update strategies}
\begin{itemize}
    \item Write-through
        \begin{itemize}
            \item[+] consistency
            \item[-] slow, write stall
        \end{itemize}
    \item Write-back
        \begin{itemize}
            \item[+] update only cache entry
            \item[+] write back to memory only when replacing cahce line
            \item[+] write only on modified, uses dirty bit for each cache line
            \item[-] However, not consistent
        \end{itemize}
\end{itemize}
These two can be combined. L1 cache uses write-though and L2 uses write-though
for example.
\subsection{Cache coherence and memory consitency}
A cache management system is called coherent if a read access to a shared memory
location
always reproduces the value coresponding to the  most recent write acces to x.
\\
A memory system is consistent (at a certain time)\ldots

Three conditions for coherence.
\begin{enumerate}
    \item Each processor sees its own writes and reads in program order
    \item The written value is eventually visible to all processors
    \item All processors see one total order of all write accesses
\end{enumerate}
\subsection{Cache coherence protocols}
\begin{itemize}
    \item Write-update protocol
        \begin{itemize}
            \item At write access, all other copies in the system must be
                updated as well. Update must finish before next access
        \end{itemize}
    \item Write-invalidate protocol
        \begin{itemize}
            \item Before modifying a copy in the cache all copies in the system
                must be declare as invalid
        \end{itemize}
\end{itemize}
Either propagate the message after update or make sure you are the only one
able to write and all other processes read from me. Most SMPs use \textbf{write-invalidate}
\\
Updating/invalidating is straightforward on bus-based systems. otherwise a directory
based mechanism is necessary.
\subsubsection{Write-invalidate in detail}
Implementation: multiple-readers-single-writer sharing.
\\Write attempt to read-only-mode data x:
\begin{enumerate}
    \item Broadcast invalidation to all other copies of x
    \item Await acks
    \item Any processor attemting to access x is blocked if a writer exists
    \item Eventually, control is transferred from the writer
        and other accesses may take place once the update has been sent
\end{enumerate}
\begin{itemize}
    \item[+] parallelism (multiple readers)
    \item[+] updates only propagated when needed
    \item[+] several updates can take place before communication is necessary
    \item[-] Cost of invalidating read-only copies before a write can occur
\end{itemize}
\subsubsection{Write-Update protocol}
\begin{itemize}
    \item Write
        \begin{itemize}
            \item done locally + broadcast new calue to all who have a copy of x
        \end{itemize}
    \item Read
        \begin{itemize}
            \item read local copy of x
        \end{itemize}
\end{itemize}
Reads are cheap, good if a lot of readers and few writers. All ok if broadcasts
can be totally ordered.

\subsubsection{Bus-snooping}
Bus controller listens for \textbf{all} messages for invalidate or broadcast messages.
The bus is a bottleneck but very good for total ordering.
\subsubsection{Write-back invalidation protocol (MSI)}
\begin{description}
    \item[M (modified)] only this cache entry is valid
    \item[S (share)] cached on one or more processors, all valie
    \item[I (invalid)] invalid copy
\end{description}
Change states based on processor operators and observed messages on the bus.
\subsubsection{MESI-protocol}
Introduce new state \textbf{E (exclusive)} no other cache has a copy of this copy
and this copy cannot be modified.
\subsubsection{CC-NUMA: Directory based Protocol for non-bus-based Architectures}
Directory keeps the copy set for each memory block. 1 precence block per memory
block and processor.
\subsection{Performance issue: false sharing}
Two variable are in same cache line but not used by both processors.
\\How to avoid?
\begin{itemize}
    \item Smaller cache lines, but more administrative effort
    \item Programmer or compiler gives hits for data placement
    \item Limit time in exclusive mode to limit ping pong
    \item Use weaker consistency models, more complicated and error-prone
\end{itemize}
\section{SIMD Architectures}
\marginpar{Lecture 3\\2016-04-06}
\subsection{Pipelined vector units}
Vectors of operands and instructions. Partition arithmetic circuits into layers, ``stages''
so that they make up a pipeline.
Requires a start-up phase, works in parallel. 
\\$N=A+B$ takes N+d-1 cycles compared to $N\times d$.
\subsection{SIMD Instructions}
True parallel instructions. Take more than one word as input, eg 128bits.
\subsection{Instruction level parallelism}
VLIW Very long instruction word. multiple functional units working in parallel.
Single string of long instruction words with explicitly parallel subinstructions.
\\
Superscalar. Multiple functional units. Single stream of orinary instructions.
\subsection{Towards multicore}
Superscalar and VLIW has reached its limits.
\section{Hardware multithreading}
Multiple threads share the functional units of a single processors.
\begin{itemize}
    \item Coarse grain - switch on load/cache miss
    \item Fine grain - switch per cycle
    \item SMT/Hyperthreading
\end{itemize}
\section{Multicore}
\subsection{Processor in memory}
Use checkerboard style with compartmentalized ``computers'' in each tile with a message
passing system on-chip. Also introduces more complexity to the programmer.

\subsection{Hardware Accelerators for HPC}
\begin{itemize}
    \item[+] Special-purpose hardware more effective than GP processor
    \item[-] Low portability
\end{itemize}
Non-reconfigurable:
\begin{itemize}
    \item AltiVec SIMD
    \item STI Cell/BE
    \item GPUs
\end{itemize}

Reconfigurable accelerators:
\begin{itemize}
    \item FPGAs
\end{itemize}
Good on integer arithmetics, compute-intensive applications with low data transfer
volume.\\
Can give:
\begin{itemize}
    \item $10^4$ speedup
    \item $10^3$ power reduction
    \item $10^2$ reduction in cost
\end{itemize}

\end{document}
