%        File: TDDC78-lecture-notes.tex
%     Created: Mon Apr 04 08:00  2016 C
% Last Change: Mon Apr 04 08:00  2016 C
%
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{hyperref}
\author{Pontus Persson}
\title{Lecture Notes\\TDDC78}
\date{VT-16}
\begin{document}
\maketitle
\tableofcontents

\marginpar{Lecture 1\\2016-04-04}
\section{Parallel Computing - Motivation}
Very large number of FLOPS. Used for example weather predictions. 10 day forecast
requires $10^{15}$ FLOPS.
\\Grand challenges:
\begin{itemize}
    \item Chemistry
    \item Physics
    \item VR
    \item Bioinformatics
\end{itemize}
Single processor computers cannot provide such massive computation power
\begin{itemize}
    \item Power/Cooling issue (freq. limited)
    \item Limited automatic parallelism (eg. at instruction level)
\end{itemize}
It is more efficient to use multiple cores instead of simply over-clocking a single core.
Given that the application can be parallelized efficiently.

\section{Parallel Computer Architecture Concepts}
\begin{description}
    \item[Multiprocessors] Tightly connected processors (eg shared memory)
    \item[Multicomputer] Loosely connected (eg distributed memory)
\end{description}
\subsection{Control Structure}
\begin{description}
    \item[SISD] single instruction stream, single data stream\\ sequential
    \item[SIMD] single instruction stream, multiple data streams\\
        Common clock, common program memory, common program counter
        \begin{itemize}
            \item VLIW processors
            \item traditional vector processors
            \item traditional array computers
            \item SIMD instructions on wide data words (e.g. Altivec, SSE, \ldots)
        \end{itemize}
    \item[MIMD] multiple instruction streams, multiple data streams. Each processors has its own program counter
    \item[Hybrid forms]
\end{description}

\subsection{Memory organization}
Memory access is a bottleneck in computing, caches are valuable!
\begin{description}
    \item[Distributed] No direct access, but through inter-connection network
        (message passing)
    \item[Shared] Direct access to memory from each processor via e.g. a bus.
\end{description}
Most common today in HPC: Hybrid memory systems. Distributed memory systems with
shared memory nodes. Clusters of servers with multicore CPUs.

\subsection{Distributed Memory Architectures}
\subsubsection{Array Computers}
\begin{itemize}
    \item SIMD
    \item Channel based, synchronous communication to neighbours
    \item Out of fashion, but comeback as SIMD accelerator arrays on chip.
\end{itemize}

\subsubsection{Multicomputers}
\begin{itemize}
    \item MIMD, async.
    \item may consist of standard PC components
    \item Very successful
\end{itemize}
Data structures must be distributed since remote memory access is very expensive.

\section{Interconnection Networks}
\begin{description}
    \item[Degree] max number of neighbours
    \item[Diameter] max distance in hops
    \item[Connectivity] fault tolerance
    \item[Bandwidth] max bytes/sec
    \item[throughput] max bytes/sec of all connections are active
\end{description}
We can:
\begin{itemize}
    \item Connect processors with each other (DMS, dynamic)
    \item Connect processors with memory modules (SMS, static)
\end{itemize}

Classification:
\begin{itemize}
    \item Direct/Static interconnection networks
    \item Direct networks with hardware routers
        \begin{itemize}
            \item Offload processors from most communication work
        \end{itemize}
    \item Switches/Dynamic interconnection networks
\end{itemize}

\subsection{Simple topology}
\begin{description}
    \item[Bus] 1 wire - bus saturated with many processors (Ethernet)
    \item[Linear Array] Like a linked list of processors.
    \item[Ring] Token ring
    \item[2D grid]
    \item[Torus] 2D grid + ring mixed
    \item[3D grid]
    \item[3D torus] 3D grid + torus mix
    \item[Tree] represent as tree, root processor is bottleneck
\end{description}
Could also use fat trees - trees with nodes interconnected to all parent level nodes.

\subsubsection{Hypercube}
A k-dimensional hypercube $C_k$ has $2^k$ nodes (connections). 
Inductive definition $C_0=$ one elem, $C_1=$line, $C_2=$square, $C_3=$cube, \ldots\\
$C_k=$two connected $C_{k-1}$ hypercubes.
\begin{itemize}
    \item Each node has a degree k (nmbr of direct neighbours)
    \item $\forall (p_i,p_j) \exists$ path of lengths $\leq k$ hops\\
        Shortest path $p_i \rightarrow^* p_j$ has length $HD(p_j,p_i)=$ \# 1-bits in 
\end{itemize}
\subsubsection{Crossbar}
Links every input to every output using one switch per connection. Total $n^2$ switches.

\begin{itemize}
    \item Very flexible
    \item High throughput
    \item Expensive
    \item Not scalable
\end{itemize}
Sometimes used in combination with other networks. Crossbars of 32 nodes connected
 with ring network.

 \section{Shared memory}
\marginpar{Lecture 2\\2016-04-05}
 \begin{itemize}
     \item SMP (Symmetric SMS) true shared
     \item SMP with cahces
     \item Distributed Shared Memory (DMS/NUMA) emulated on top of distributed memory
 \end{itemize}
 \section{Cache}
 Small, fast memory between processor and main memory.
 \begin{description}
     \item[cache hit] accessed work is in cache
     \item[cache miss] not in cache
     \item[cache line] holds a copy of a block of adhjacent memory words
         \begin{itemize}
             \item size: from 16 bytes and upwards
         \end{itemize}
 \end{description}
Time to access n bytes: $t(n)=\beta+\alpha n$. Depending on $\alpha$ and $\beta$,
accessing more bytes might be more efficient.
\\Mapping memory blocks to cache lines:
directly mapped $\forall j\exists!i: B_j\rightarrow C_i$ namely where\ldots
\begin{itemize}
    \item Spatial access locality
        \begin{itemize}
            \item access also other data in same cache line
        \end{itemize}
        \item temporal access locality
            \begin{itemize}
                \item access same location multiple times
            \end{itemize}
        \item HW-controlled cache line replacement
            \begin{itemize}
                \item LRU - least recently used - dynamic adaptivity of cache contents
            \end{itemize}
        \item Suitable for applications with high or dynamic data locality
\end{itemize}
\subsection{Cache misses}
\begin{description}
    \item[Cold miss] data was never in cache
        \begin{itemize}
            \item Usually unavoidable
        \end{itemize}
    \item[Capacity miss] data was evicted earlier when cache capacity was exceeded
        \begin{itemize}
            \item Might be partly avoided
        \end{itemize}
    \item[Associativity miss] data was evicted earlier because of cache set conflict
\end{description}
\subsection{Memory update strategies}
\begin{itemize}
    \item Write-through
        \begin{itemize}
            \item[+] consistency
            \item[-] slow, write stall
        \end{itemize}
    \item Write-back
        \begin{itemize}
            \item[+] update only cache entry
            \item[+] write back to memory only when replacing cahce line
            \item[+] write only on modified, uses dirty bit for each cache line
            \item[-] However, not consistent
        \end{itemize}
\end{itemize}
These two can be combined. L1 cache uses write-though and L2 uses write-though
for example.
\subsection{Cache coherence and memory consitency}
A cache management system is called coherent if a read access to a shared memory
location
always reproduces the value coresponding to the  most recent write acces to x.
\\
A memory system is consistent (at a certain time)\ldots

Three conditions for coherence.
\begin{enumerate}
    \item Each processor sees its own writes and reads in program order
    \item The written value is eventually visible to all processors
    \item All processors see one total order of all write accesses
\end{enumerate}
\subsection{Cache coherence protocols}
\begin{itemize}
    \item Write-update protocol
        \begin{itemize}
            \item At write access, all other copies in the system must be
                updated as well. Update must finish before next access
        \end{itemize}
    \item Write-invalidate protocol
        \begin{itemize}
            \item Before modifying a copy in the cache all copies in the system
                must be declare as invalid
        \end{itemize}
\end{itemize}
Either propagate the message after update or make sure you are the only one
able to write and all other processes read from me. Most SMPs use \textbf{write-invalidate}
\\
Updating/invalidating is straightforward on bus-based systems. otherwise a directory
based mechanism is necessary.
\subsubsection{Write-invalidate in detail}
Implementation: multiple-readers-single-writer sharing.
\\Write attempt to read-only-mode data x:
\begin{enumerate}
    \item Broadcast invalidation to all other copies of x
    \item Await acks
    \item Any processor attemting to access x is blocked if a writer exists
    \item Eventually, control is transferred from the writer
        and other accesses may take place once the update has been sent
\end{enumerate}
\begin{itemize}
    \item[+] parallelism (multiple readers)
    \item[+] updates only propagated when needed
    \item[+] several updates can take place before communication is necessary
    \item[-] Cost of invalidating read-only copies before a write can occur
\end{itemize}
\subsubsection{Write-Update protocol}
\begin{itemize}
    \item Write
        \begin{itemize}
            \item done locally + broadcast new calue to all who have a copy of x
        \end{itemize}
    \item Read
        \begin{itemize}
            \item read local copy of x
        \end{itemize}
\end{itemize}
Reads are cheap, good if a lot of readers and few writers. All ok if broadcasts
can be totally ordered.

\subsubsection{Bus-snooping}
Bus controller listens for \textbf{all} messages for invalidate or broadcast messages.
The bus is a bottleneck but very good for total ordering.
\subsubsection{Write-back invalidation protocol (MSI)}
\begin{description}
    \item[M (modified)] only this cache entry is valid
    \item[S (share)] cached on one or more processors, all valie
    \item[I (invalid)] invalid copy
\end{description}
Change states based on processor operators and observed messages on the bus.
\subsubsection{MESI-protocol}
Introduce new state \textbf{E (exclusive)} no other cache has a copy of this copy
and this copy cannot be modified.
\subsubsection{CC-NUMA: Directory based Protocol for non-bus-based Architectures}
Directory keeps the copy set for each memory block. 1 precence block per memory
block and processor.
\subsection{Performance issue: false sharing}
Two variable are in same cache line but not used by both processors.
\\How to avoid?
\begin{itemize}
    \item Smaller cache lines, but more administrative effort
    \item Programmer or compiler gives hits for data placement
    \item Limit time in exclusive mode to limit ping pong
    \item Use weaker consistency models, more complicated and error-prone
\end{itemize}
\section{SIMD Architectures}
\marginpar{Lecture 3\\2016-04-06}
\subsection{Pipelined vector units}
Vectors of operands and instructions. Partition arithmetic circuits into layers, ``stages''
so that they make up a pipeline.
Requires a start-up phase, works in parallel. 
\\$N=A+B$ takes N+d-1 cycles compared to $N\times d$.
\subsection{SIMD Instructions}
True parallel instructions. Take more than one word as input, eg 128bits.
\subsection{Instruction level parallelism}
VLIW Very long instruction word. multiple functional units working in parallel.
Single string of long instruction words with explicitly parallel subinstructions.
\\
Superscalar. Multiple functional units. Single stream of orinary instructions.
\subsection{Towards multicore}
Superscalar and VLIW has reached its limits.
\section{Hardware multithreading}
Multiple threads share the functional units of a single processors.
\begin{itemize}
    \item Coarse grain - switch on load/cache miss
    \item Fine grain - switch per cycle
    \item SMT/Hyperthreading
\end{itemize}
\section{Multicore}
\subsection{Processor in memory}
Use checkerboard style with compartmentalized ``computers'' in each tile with a message
passing system on-chip. Also introduces more complexity to the programmer.

\subsection{Hardware Accelerators for HPC}
\begin{itemize}
    \item[+] Special-purpose hardware more effective than GP processor
    \item[-] Low portability
\end{itemize}
Non-reconfigurable:
\begin{itemize}
    \item AltiVec SIMD
    \item STI Cell/BE
    \item GPUs
\end{itemize}

Reconfigurable accelerators:
\begin{itemize}
    \item FPGAs
\end{itemize}
Good on integer arithmetics, compute-intensive applications with low data transfer
volume.\\
Can give:
\begin{itemize}
    \item $10^4$ speedup
    \item $10^3$ power reduction
    \item $10^2$ reduction in cost
\end{itemize}

\section{Design of parallel programs}
\marginpar{Lecture 4\\2016-04-08}
Approaches:
\begin{itemize}
    \item Message passing
    \item Shared memory programming
    \item Dataparallel programming
    \item PCAM
\begin{itemize}
    \item Partitioning
    \item Communication
    \item Agglomeration
    \item Mapping
\end{itemize}
\end{itemize}
\subsection{Parallel computiation models}
Abstract from hardware and technology, specify basic operations, specify
how data is stored. Work in terms of psuedo code instead of source code
to make code more portable.\\
A model consists of:
\begin{itemize}
    \item Programming model (qual)
        \begin{itemize}
            \item basic operations
            \item memory model
            \item degree of synchronous execution (how much synch must be done)
        \end{itemize}
    \item Cost model (quant)
        \begin{itemize}
            \item Key parameters
            \item cost functions for basic operations (what is cost of arithmetic
                operations, branching, etc.)
            \item constraints (how many send in network before software layer
                is saturated, etc.)
        \end{itemize}
\end{itemize}
Analyze algorithms before implementation. Focus on characteristic influence on time/
space complexity features of broader class of parallel machines.

\subsection{The notion of task}
A parallel computation consists of tasks. Tasks execute concurrently.\\
4 basic operations + local operations:
\begin{enumerate}
    \item send a message
    \item recv a message
    \item create new task
    \item terminate
\end{enumerate}
Tasks can be mapped to physical processors (assigned for execution). Tasks
define a notion of locality: items in local memory are ``close'' other are
``remote''.
\subsection{Message passing}
\subsubsection{Via channels and ports}
Messages sent via channels = message queues, mailboxes. Typically send is
non-blocking and receive is a blocking operation.
\subsubsection{Point to point}
Communication to named process (unique name, PID). Equivalent to channels.
\subsection{Shared memory}
Tasks share a common address space. Communication between tasks via shared data
via locks and semaphores. No data distribution, no notion of ownership.
\begin{itemize}
    \item[+] Easier program development
    \item[+] No need to specify communication of data
    \item[$-$] Hard to understand and manage locality
        \begin{itemize}
            \item[$\rightarrow$] performance tuning more difficult than message
                passing
        \end{itemize}
\end{itemize}
\subsection{Dataparallel programming}
Concurrency stems from applying the same operation to multiple elements of the same
data structure. ``Add 2 to alla elements of this array'', ``Increase the salaryof all employees with 5 years service''. 
\\Large amount of fine granularity tasks:
of all employees with 5 years service''. Large amount of fine granularity tasks:
\begin{itemize}
    \item Allows to extract massive parallelism for large data sets
    \item Code easier to understand
    \item Communication can be figured out automatically by compier
        \begin{itemize}
            \item[$\rightarrow$] easier to program than message passing
        \end{itemize}
    \item Share address space from programmers pov
    \item May be executed on SIMD or MIMD architecture
\end{itemize}
\subsection{Foster's PCAM method for parallel program design}
$C=AB$ $C_{i,j}=\sum a_{i,k}b_{kj} \forall i,j\in\left\{ 1,\cdots n \right\}$.
Sequential approach: three nested for-loops.\\
\begin{enumerate}
    \item Partition - what is the smallest computation step?
    \item Add communication, synchronization, constraints, etc.
    \item[$\rightarrow$] This is for ideal machine, specify for specific parallel computer
    \item Agglomerate (add tasks to macro-tasks, executes contained tasks
        sequentially)
    \item Map macro-tasks/processes to processors
\end{enumerate}
\subsubsection{Partition}
Try to decompose program into pieces, tasks.\\
Two main methods:
\begin{itemize}
    \item Domain decomposition
        \begin{itemize}
            \item partition data structures
            \item one task per parition
        \end{itemize}
    \item functional decomposition
        \begin{itemize}
            \item one task per function call or module execution
        \end{itemize}
\end{itemize}
\textbf{Example} - domain decomposition:
\begin{verbatim}
for all i,j,k from 1 to n
A[i,j,k] = f (A[i-1,j,k], A[i,j-1,k], \ldots, A(i,j,k+i);
\end{verbatim}
may be decomposed into 1,2 or 3 dimensions. 3D-decomposition offers the greatest
flexibility.
\\\textbf{Example}: functional decomposition\\
N-queens, alpha-beta search etc. A new task is created for each search call
(function call). Functions not located on the same path may execute in parallel.
\\
\textbf{Example}: functional model decomposition\\
Simulation of earth climate. One task per model (atmosphere, ocean, land, hydrology).
A coarse function decomposition according to the components.
\\
\textbf{Checklist:}
\begin{itemize}
    \item Define more tasks than there are processors
    \item Avoid redundant computation and storage
    \item Make tasks of comparable size
    \item number of tasks should scale with problem size
    \item Try different alternative partitions
\end{itemize}
\subsubsection{Communication and Synchronization}
Determine communication and synchronization structure. can be static/dynamic and
local/global. Local communication (update value on weighted average of neighbours
in 2D-mesh). This is called Jacobi-update schema.\\
\\Gauss-Seidel update schema: Use old values for north and west and new values for
south and east.
\\Faster convergence than Jacobi, but less parallelism.
\\
\\Example for global communication: summing a sequence of numbers.\\
Try to use divide and conquer, for sum, use binary tree.\\
\\Unstructured and dynamic communication: Communication structure may be irregulr
and data dependent, and may change dynamically during execution. (heat sink).

\subsubsection{Pipelining}
Complete sequence of dependant computation (f1 to fk) to be applied elementwise
on x1 to xn.  For xi fi must be done before fi-1. fi on different x are independant.
\\Overlap execution of all fi for k subsequent xj.

\subsubsection{Communication and synchronization checklist}
\begin{itemize}
    \item Tasks should perform approx. equal numbers of communication
    \item Communicate with small number of neighbours
    \item Communication should be able to proceed concurrently, otherwise does not scale
    \item Computation assoiciated with different tasks should be able to proceed concurrently
\end{itemize}
\marginpar{Lecture 5\\2016-04-11}
\subsubsection{Step 3: Agglomeration}
Tries to combine, merge, agglomerate tasks to obtain fewer tasks each of greater
size.
\begin{itemize}
    \item Trade-off between concurrency and communication.
\end{itemize}
\begin{itemize}
    \item Reduce from 3 dimensions to 2
    \item Combine adjacent tasks into 3d partitions
    \item Subtrees in D\&C are coalesced.
    \item Nodes in three algorithm are combined
\end{itemize}
In a 2D mesh: exploit surface to volume ratio, replicate computation to
reduce communication. Bundle multiple communications from same sender to same
receiver in one message.
\subsubsection{Step 4: Mapping and scheduling}
Specify where each task should execute (mapping). Scheduling: where and when
should task execute?
\\Goal: Minimize total execution time.
\\Strategies:
\begin{itemize}
    \item Place tasks (if possible) on different processors
    \item Place tasks that communicate frequently on the same processor
    \item Load balancing
\end{itemize}
Determined statically of dynamically.
\section{MPI - Message Passing Interface}
SPMD. Constant number of parallel activities in contrast to fork/join.
\subsection{Determinism}
\begin{itemize}
    \item Arrival order of tho sent messages is unspecified
    \item Guarantees that two messages sent from A to B will arrive in the same
        order
    \item Messages can be distinguished by sender and a tag
\end{itemize}
\marginpar{Lecture 6\\2016-04-12}
\subsection{MPI functions}
When using reduce, there might be better functions to used depending on the
MPI implementation (bluegeene).
\section{Using HPC systems at NSC}
Use module: buildenv-indel/2015-1
\\-Nmpi, -openmp\\
\marginpar{Lecture 7\\2016-04-15}
\section{OpenMP}
The slides are boss, no need to take notes
\marginpar{Lecture 8\\2016-04-20}
\subsection{Task}
A task has:
\begin{itemize}
    \item Code to execute
    \item A data environment (owns the data)
    \item An assigned thread that executes the code and uses the data
\end{itemize}
\end{document}
